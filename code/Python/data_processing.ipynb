{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033c608-00b7-4287-ba0e-beafc35b3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re # For more robust number extraction\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from mpl_toolkits.mplot3d import Axes3D # For 3D plotting\n",
    "import numpy as np # For potential numerical operations, e.g. np.nan\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a692cf-470f-4c83-ba44-8af9f0fcc74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_log_files(data_folder_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads all LOG_*.TXT files from the specified folder, sorts them numerically,\n",
    "    and combines them into a single Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data_folder_path (str): The path to the folder containing the .txt log files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all combined data, or an empty\n",
    "                      DataFrame if no suitable files are found or an error occurs.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(data_folder_path):\n",
    "        print(f\"Error: Folder '{data_folder_path}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_files_in_folder = glob.glob(os.path.join(data_folder_path, \"*\"))\n",
    "    \n",
    "    txt_files = []\n",
    "    for f_path in all_files_in_folder:\n",
    "        filename = os.path.basename(f_path)\n",
    "        if filename.upper().startswith(\"LOG_\") and filename.upper().endswith(\".TXT\"):\n",
    "            txt_files.append(f_path)\n",
    "\n",
    "    if not txt_files:\n",
    "        print(f\"No LOG_*.TXT files found in '{data_folder_path}'.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    def sort_key(filepath):\n",
    "        filename = os.path.basename(filepath)\n",
    "        match = re.search(r'LOG_(\\d+)\\.TXT', filename, re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        return float('inf') \n",
    "\n",
    "    sorted_files = sorted(txt_files, key=sort_key)\n",
    "    \n",
    "    print(\"Files to be processed in order:\")\n",
    "    for f in sorted_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "    all_dataframes = []\n",
    "    for file_path in sorted_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"Successfully read and processed: {os.path.basename(file_path)}\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"Warning: File {os.path.basename(file_path)} is empty and will be skipped.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "    if not all_dataframes:\n",
    "        print(\"No data could be read from the files.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fcb9b-e92f-4384-975c-e4b49c11cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"Data_In\" \n",
    "output_folder = \"Data_Out\" # Define the output folder\n",
    "\n",
    "\n",
    "combined_data = combine_log_files(input_folder)\n",
    "df = combined_data\n",
    "if not combined_data.empty:\n",
    "    print(\"\\n--- Combined DataFrame ---\")\n",
    "    print(\"Shape:\", combined_data.shape)\n",
    "    print(\"\\nHead:\")\n",
    "    print(combined_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ecfc81-88ed-408c-b803-69d2c667ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_folder, exist_ok=True)\n",
    "print(f\"\\nSaving plots to folder: {output_folder}\")\n",
    "\n",
    "# Apply a publication-ready style\n",
    "style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# --- Data Preparation for Plot 1 Time Axis ---\n",
    "df_plotting = df.copy()\n",
    "\n",
    "gps_datetime_cols = ['GPS_Year', 'GPS_Month', 'GPS_Day', 'GPS_Hour', 'GPS_Min', 'GPS_Sec']\n",
    "for col in gps_datetime_cols:\n",
    "    df_plotting[col] = pd.to_numeric(df_plotting[col], errors='coerce')\n",
    "\n",
    "df_plotting['GPS_DateTime_Valid'] = pd.NaT\n",
    "\n",
    "valid_gps_mask = (\n",
    "    (df_plotting['GPS_Fix'] == 1) &\n",
    "    (df_plotting['GPS_Year'] > 0) &\n",
    "    (df_plotting['GPS_Month'] >= 1) & (df_plotting['GPS_Month'] <= 12) &\n",
    "    (df_plotting['GPS_Day'] >= 1) & (df_plotting['GPS_Day'] <= 31) &\n",
    "    (df_plotting['GPS_Hour'] >= 0) & (df_plotting['GPS_Hour'] <= 23) &\n",
    "    (df_plotting['GPS_Min'] >= 0) & (df_plotting['GPS_Min'] <= 59) &\n",
    "    (df_plotting['GPS_Sec'] >= 0) & (df_plotting['GPS_Sec'] <= 59)\n",
    ")\n",
    "\n",
    "datetime_constructor_df = pd.DataFrame({\n",
    "    'year': df_plotting.loc[valid_gps_mask, 'GPS_Year'],\n",
    "    'month': df_plotting.loc[valid_gps_mask, 'GPS_Month'],\n",
    "    'day': df_plotting.loc[valid_gps_mask, 'GPS_Day'],\n",
    "    'hour': df_plotting.loc[valid_gps_mask, 'GPS_Hour'],\n",
    "    'minute': df_plotting.loc[valid_gps_mask, 'GPS_Min'],\n",
    "    'second': df_plotting.loc[valid_gps_mask, 'GPS_Sec']\n",
    "})\n",
    "\n",
    "if not datetime_constructor_df.empty:\n",
    "    # Ensure all columns are numeric before passing to to_datetime\n",
    "    for col in datetime_constructor_df.columns:\n",
    "        datetime_constructor_df[col] = pd.to_numeric(datetime_constructor_df[col], errors='coerce')\n",
    "    \n",
    "    # Drop rows with any NaNs that might have been introduced by coerce, before to_datetime\n",
    "    datetime_constructor_df.dropna(inplace=True)\n",
    "\n",
    "    if not datetime_constructor_df.empty:\n",
    "         # Assign back to the original DataFrame's filtered view (df_plotting.loc[valid_gps_mask])\n",
    "         # The index of datetime_constructor_df matches the True values in valid_gps_mask\n",
    "        df_plotting.loc[datetime_constructor_df.index, 'GPS_DateTime_Valid'] = pd.to_datetime(datetime_constructor_df, errors='coerce')\n",
    "\n",
    "\n",
    "df_plotting['Plot_Time'] = pd.NaT\n",
    "\n",
    "if not df_plotting['GPS_DateTime_Valid'].isnull().all():\n",
    "    first_valid_dt_idx = df_plotting['GPS_DateTime_Valid'].first_valid_index()\n",
    "    if first_valid_dt_idx is not None:\n",
    "        df_plotting.loc[first_valid_dt_idx, 'Plot_Time'] = df_plotting.loc[first_valid_dt_idx, 'GPS_DateTime_Valid']\n",
    "        # Backfill time for rows before the first valid GPS time\n",
    "        for i in range(first_valid_dt_idx - 1, -1, -1):\n",
    "            # Check if previous Plot_Time is already set (e.g. by another valid GPS point)\n",
    "            # This check is mostly for robustness, standard loop should be fine\n",
    "            if pd.isna(df_plotting.loc[i+1, 'Plot_Time']): # Should not happen in this loop\n",
    "                break \n",
    "            df_plotting.loc[i, 'Plot_Time'] = df_plotting.loc[i + 1, 'Plot_Time'] - pd.Timedelta(seconds=1)\n",
    "        \n",
    "        # Fill subsequent NaNs or use GPS time\n",
    "        for i in range(first_valid_dt_idx + 1, len(df_plotting)):\n",
    "            if pd.notna(df_plotting.loc[i, 'GPS_DateTime_Valid']):\n",
    "                df_plotting.loc[i, 'Plot_Time'] = df_plotting.loc[i, 'GPS_DateTime_Valid']\n",
    "            else:\n",
    "                if pd.isna(df_plotting.loc[i-1, 'Plot_Time']): # If previous is also NaN, cannot increment\n",
    "                     # This could happen if there's a large gap of non-GPS data after first_valid_dt_idx\n",
    "                     # and before another valid GPS_DateTime_Valid.\n",
    "                     # For now, let it be NaT, to be caught by the manual input if all Plot_Time is NaT.\n",
    "                     # Or, one could decide to keep incrementing.\n",
    "                     # If this creates an issue, this logic might need refinement for sparse GPS data.\n",
    "                     pass # Keep as NaT\n",
    "                else:\n",
    "                     df_plotting.loc[i, 'Plot_Time'] = df_plotting.loc[i-1, 'Plot_Time'] + pd.Timedelta(seconds=1)\n",
    "    # else: # first_valid_dt_idx is None, but not all GPS_DateTime_Valid are null. This is unlikely.\n",
    "        # If this happens, Plot_Time will remain all NaT, and the manual input logic will trigger.\n",
    "    #    pass\n",
    "\n",
    "# MODIFIED SECTION FOR MANUAL TIME INPUT\n",
    "if df_plotting['Plot_Time'].isnull().all():\n",
    "    print(\"Warning: No valid GPS time found in any row to anchor the timeline, or issues populating Plot_Time.\")\n",
    "    print(\"The time axis for plots will be based on a manually entered start time.\")\n",
    "    while True:\n",
    "        manual_start_time_str = input(\"Please enter the start datetime of the experiment (e.g., 'YYYY-MM-DD HH:MM:SS'): \")\n",
    "        try:\n",
    "            start_time = pd.to_datetime(manual_start_time_str)\n",
    "            print(f\"Using manually entered start time: {start_time} for the time axis.\")\n",
    "            df_plotting['Plot_Time'] = [start_time + pd.Timedelta(seconds=i) for i in range(len(df_plotting))]\n",
    "            break \n",
    "        except ValueError:\n",
    "            print(\"Invalid datetime format. Please use a format like 'YYYY-MM-DD HH:MM:SS' (e.g., '2023-01-01 14:30:00'). Try again.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}. Please check your input and try again.\")\n",
    "\n",
    "df_plotting['BMP_PressAtm'] = df_plotting['BMP_PressPa'] / 101325.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ff4b3-bc93-4ac4-932a-06e926d6bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Time Series Subplots ---\n",
    "print(\"Generating Plot 1: Time Series Data...\")\n",
    "fig1, axes1 = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "fig1.suptitle('Sensor Time Series Data', fontsize=16, y=0.99)\n",
    "\n",
    "axes1[0].plot(df_plotting['Plot_Time'], df_plotting['SHT_TempC'], label='SHT Temp', color='crimson')\n",
    "axes1[0].set_ylabel('Temperature (°C)')\n",
    "axes1[0].legend(loc='upper right')\n",
    "axes1[0].set_title('Ambient Temperature (SHT85)')\n",
    "\n",
    "axes1[1].plot(df_plotting['Plot_Time'], df_plotting['SHT_Hum_%'], label='SHT Humidity', color='royalblue')\n",
    "axes1[1].set_ylabel('Humidity (%)')\n",
    "axes1[1].legend(loc='upper right')\n",
    "axes1[1].set_title('Ambient Humidity (SHT85)')\n",
    "\n",
    "axes1[2].plot(df_plotting['Plot_Time'], df_plotting['BMP_PressAtm'], label='BMP Pressure', color='green')\n",
    "axes1[2].set_ylabel('Pressure (atm)')\n",
    "axes1[2].legend(loc='upper right')\n",
    "axes1[2].set_title('Atmospheric Pressure (BMP585)')\n",
    "\n",
    "axes1[3].plot(df_plotting['Plot_Time'], df_plotting['CO2_ppm'], label='CO2', color='purple')\n",
    "axes1[3].set_ylabel('CO2 (ppm)')\n",
    "axes1[3].set_xlabel('Time')\n",
    "axes1[3].legend(loc='upper right')\n",
    "axes1[3].set_title('CO2 Concentration (SCD30)')\n",
    "\n",
    "if pd.api.types.is_datetime64_any_dtype(df_plotting['Plot_Time']):\n",
    "    fig1.autofmt_xdate() \n",
    "    axes1[3].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plot1_path = os.path.join(output_folder, \"plot1_time_series.png\")\n",
    "plt.savefig(plot1_path)\n",
    "print(f\"Saved: {plot1_path}\")\n",
    "plt.show() # MODIFIED: Show plot\n",
    "# plt.close(fig1) # MODIFIED: Commented out to show plot in Jupyter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78374e1c-8231-4c28-8471-73d68e2baac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f7414-29e3-4b96-a279-629efec849da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing ---\n",
    "print(\"Preprocessing data: calculating H2O ppm, removing outliers, and preparing for aggregation...\")\n",
    "df_processed = df_plotting.copy()\n",
    "\n",
    "# Ensure Plot_Time is datetime and set as index\n",
    "if not pd.api.types.is_datetime64_any_dtype(df_processed['Plot_Time']):\n",
    "    df_processed['Plot_Time'] = pd.to_datetime(df_processed['Plot_Time'])\n",
    "df_processed = df_processed.set_index('Plot_Time')\n",
    "\n",
    "# --- *** ADDED: Calculate Water Vapor Concentration (ppm) *** ---\n",
    "# Constants\n",
    "ATM_TO_KPA = 101.325 # Conversion factor from atm to kPa\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols_for_ppm = ['SHT_TempC', 'SHT_Hum_%', 'BMP_PressAtm']\n",
    "if all(col in df_processed.columns for col in required_cols_for_ppm):\n",
    "    print(\"Calculating Water Vapor Concentration (H2O ppm)...\")\n",
    "    # Extract data (use .values for slight speedup with numpy, though pandas ops are fine too)\n",
    "    T_C = df_processed['SHT_TempC']\n",
    "    RH_percent = df_processed['SHT_Hum_%']\n",
    "    P_atm = df_processed['BMP_PressAtm']\n",
    "\n",
    "    # 1. Calculate Saturation Vapor Pressure (es) in kPa using Magnus-Tetens approximation\n",
    "    # es(T) = 0.6108 * exp((17.27 * T) / (T + 237.3))  (T in Celsius, es in kPa)\n",
    "    es_kPa = 0.6108 * np.exp((17.27 * T_C) / (T_C + 237.3))\n",
    "\n",
    "    # 2. Calculate Actual Vapor Pressure (e) in kPa\n",
    "    # e = (RH / 100) * es\n",
    "    e_kPa = (RH_percent / 100.0) * es_kPa\n",
    "\n",
    "    # 3. Convert Total Atmospheric Pressure (P) to kPa\n",
    "    P_total_kPa = P_atm * ATM_TO_KPA\n",
    "\n",
    "    # 4. Calculate Water Vapor Concentration in ppm (volume/volume)\n",
    "    # ppm_v = (e / P_total) * 1,000,000\n",
    "    # Ensure P_total_kPa is not zero to avoid division errors (though unlikely for atm pressure)\n",
    "    # Replace potential division by zero or NaN pressure with NaN result\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): # Suppress warnings for division by zero/NaN\n",
    "         df_processed['H2O_ppm'] = (e_kPa / P_total_kPa) * 1e6\n",
    "         df_processed.loc[P_total_kPa <= 0, 'H2O_ppm'] = np.nan # Handle zero/negative pressure explicitly\n",
    "\n",
    "    print(\"H2O ppm calculation complete.\")\n",
    "    h2o_ppm_calculated = True\n",
    "else:\n",
    "    print(\"Warning: Cannot calculate H2O ppm. Missing one or more required columns: \", required_cols_for_ppm)\n",
    "    h2o_ppm_calculated = False\n",
    "    # Add a dummy column with NaNs if needed later, although outlier/aggregation will handle absence\n",
    "    if 'H2O_ppm' not in df_processed.columns:\n",
    "         df_processed['H2O_ppm'] = np.nan\n",
    "# --- *** END ADDED SECTION *** ---\n",
    "\n",
    "\n",
    "# Columns to process (updated to include H2O_ppm if calculated)\n",
    "# We still keep SHT_Hum_% here if needed for other potential processing,\n",
    "# but we won't plot it in the time series below. Add H2O_ppm.\n",
    "sensor_columns = ['SHT_TempC', 'SHT_Hum_%', 'BMP_PressAtm', 'CO2_ppm']\n",
    "if h2o_ppm_calculated and 'H2O_ppm' not in sensor_columns:\n",
    "     sensor_columns.append('H2O_ppm') # Add the new column for processing\n",
    "\n",
    "# 1. Outlier Removal (using IQR method per column)\n",
    "print(\"Removing outliers...\")\n",
    "for col in sensor_columns:\n",
    "    if col in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[col]): # Check if column exists and is numeric\n",
    "        Q1 = df_processed[col].quantile(0.25)\n",
    "        Q3 = df_processed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Replace outliers with NaN. NaNs will be ignored by mean/std in resampling.\n",
    "        original_count = df_processed[col].count() # Count non-NaN before masking\n",
    "        df_processed[col] = df_processed[col].mask(\n",
    "            (df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)\n",
    "        )\n",
    "        nan_count = original_count - df_processed[col].count() # Count NaNs introduced\n",
    "        # The original outlier calculation was slightly off, comparing df_plotting to df_processed bounds\n",
    "        # This version calculates outliers identified within df_processed itself\n",
    "        print(f\"Identified and marked {nan_count} outliers as NaN in '{col}'.\")\n",
    "    elif col not in df_processed.columns:\n",
    "         print(f\"Skipping outlier removal for '{col}': Column not found.\")\n",
    "    else:\n",
    "         print(f\"Skipping outlier removal for '{col}': Column is not numeric.\")\n",
    "\n",
    "\n",
    "# 2. Averaging for 1 minute and calculating standard deviation\n",
    "print(\"Aggregating data to 1-minute intervals (mean and std)...\")\n",
    "# The '1T' or '1min' string means 1-minute frequency for resampling.\n",
    "# .agg will compute both mean and std. NaN values are skipped by default.\n",
    "# Ensure we only try to aggregate columns that actually exist in df_processed\n",
    "columns_to_aggregate = [col for col in sensor_columns if col in df_processed.columns]\n",
    "if columns_to_aggregate:\n",
    "    df_aggregated = df_processed[columns_to_aggregate].resample('2min').agg(['mean', 'std'])\n",
    "else:\n",
    "    print(\"Error: No columns available for aggregation.\")\n",
    "    # Handle error appropriately, maybe exit or create an empty df_aggregated\n",
    "    df_aggregated = pd.DataFrame() # Create empty df to avoid downstream errors\n",
    "\n",
    "# df_aggregated will now have multi-level columns, e.g., ('SHT_TempC', 'mean') and ('SHT_TempC', 'std')\n",
    "\n",
    "# --- 1) Time Series Subplots (Modified) ---\n",
    "print(\"Generating Plot 1: Time Series Data (2-min average with std bands)...\")\n",
    "fig1, axes1 = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "fig1.suptitle('Sensor Time Series Data (2-min Average ±1 Std Dev)', fontsize=16, y=0.99)\n",
    "\n",
    "# --- *** MODIFIED: Plot Configuration *** ---\n",
    "# Replace SHT_Hum_% plot config with H2O_ppm config\n",
    "plot_configs = [\n",
    "    {'col': 'SHT_TempC', 'label': 'SHT Temp', 'color': 'crimson', 'ylabel': 'Temperature (°C)', 'title': 'Ambient Temperature (SHT85)'},\n",
    "    {'col': 'H2O_ppm', 'label': 'Water Vapor', 'color': 'deepskyblue', 'ylabel': 'H2O (ppm)', 'title': 'Water Vapor Concentration'}, # MODIFIED ROW\n",
    "    {'col': 'BMP_PressAtm', 'label': 'BMP Pressure', 'color': 'green', 'ylabel': 'Pressure (atm)', 'title': 'Atmospheric Pressure (BMP585)'},\n",
    "    {'col': 'CO2_ppm', 'label': 'CO2', 'color': 'purple', 'ylabel': 'CO2 (ppm)', 'title': 'CO2 Concentration (SCD30)'}\n",
    "]\n",
    "# --- *** END MODIFIED SECTION *** ---\n",
    "\n",
    "\n",
    "for i, config in enumerate(plot_configs):\n",
    "    ax = axes1[i]\n",
    "    col_name = config['col']\n",
    "\n",
    "    # Check if the specific column (mean and std) exists in the aggregated data\n",
    "    mean_col = (col_name, 'mean')\n",
    "    std_col = (col_name, 'std')\n",
    "\n",
    "    if mean_col not in df_aggregated.columns or std_col not in df_aggregated.columns:\n",
    "        # Provide more specific feedback if the column wasn't calculated or aggregated\n",
    "        if col_name == 'H2O_ppm' and not h2o_ppm_calculated:\n",
    "             print(f\"Warning: Cannot plot {col_name}. Data was not calculated (missing inputs).\")\n",
    "             ax.set_title(f\"{config['title']} (Input Data Missing)\")\n",
    "        elif col_name not in columns_to_aggregate:\n",
    "             print(f\"Warning: Cannot plot {col_name}. Column was not found in processed data before aggregation.\")\n",
    "             ax.set_title(f\"{config['title']} (Column Not Found)\")\n",
    "        else:\n",
    "             print(f\"Warning: Mean or std for {col_name} not found in aggregated data. Skipping plot.\")\n",
    "             ax.set_title(f\"{config['title']} (Aggregation Error?)\")\n",
    "\n",
    "        ax.set_ylabel(config['ylabel'])\n",
    "        if i == len(plot_configs) - 1: ax.set_xlabel('Time')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6) # Add grid for readability\n",
    "        continue\n",
    "\n",
    "    # Extract mean and std series\n",
    "    mean_series = df_aggregated[mean_col].dropna() # Drop rows where mean is NaN\n",
    "    std_series = df_aggregated[std_col].reindex(mean_series.index).fillna(0) # Align and fill NaN std with 0\n",
    "\n",
    "    if mean_series.empty:\n",
    "        print(f\"Warning: No data to plot for {col_name} after aggregation and NaN removal.\")\n",
    "        ax.set_title(f\"{config['title']} (No data after aggregation)\")\n",
    "        ax.set_ylabel(config['ylabel'])\n",
    "        if i == len(plot_configs) - 1: ax.set_xlabel('Time')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        continue\n",
    "\n",
    "    # Plot the mean\n",
    "    ax.plot(mean_series.index, mean_series, label=config['label'], color=config['color'], linewidth=1.5)\n",
    "\n",
    "    # Plot the standard deviation bands\n",
    "    ax.fill_between(mean_series.index,\n",
    "                    mean_series - std_series,\n",
    "                    mean_series + std_series,\n",
    "                    color=config['color'],\n",
    "                    alpha=0.2, # Semi-transparent\n",
    "                    label='±1 Std Dev' if i == 0 else \"_nolegend_\") # Label std dev band only once\n",
    "\n",
    "    ax.set_ylabel(config['ylabel'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(config['title'])\n",
    "    ax.grid(True, linestyle='--', alpha=0.6) # Add grid for readability\n",
    "\n",
    "# Set common x-label only for the last plot\n",
    "axes1[-1].set_xlabel('Time')\n",
    "\n",
    "# Apply date formatting to the x-axis\n",
    "if not df_aggregated.empty and pd.api.types.is_datetime64_any_dtype(df_aggregated.index):\n",
    "    fig1.autofmt_xdate()\n",
    "    # Use AutoDateLocator and ConciseDateFormatter for better automatic formatting\n",
    "    locator = mdates.AutoDateLocator(minticks=5, maxticks=10)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    axes1[-1].xaxis.set_major_locator(locator)\n",
    "    axes1[-1].xaxis.set_major_formatter(formatter)\n",
    "    # axes1[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M')) # Alternative explicit format\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust rect to prevent title overlap\n",
    "\n",
    "# Ensure output folder exists before saving\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "plot1_path = os.path.join(output_folder, \"plot1_time_series_aggregated.png\")\n",
    "plt.savefig(plot1_path)\n",
    "print(f\"Saved: {plot1_path}\")\n",
    "plt.show()\n",
    "# plt.close(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba2a72-ed40-4105-8b6a-e089f9a88ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing ---\n",
    "print(\"Preprocessing data: calculating H2O ppm, removing outliers, and preparing for aggregation...\")\n",
    "df_processed = df_plotting.copy()\n",
    "\n",
    "# Ensure Plot_Time is datetime and set as index\n",
    "if not pd.api.types.is_datetime64_any_dtype(df_processed['Plot_Time']):\n",
    "    df_processed['Plot_Time'] = pd.to_datetime(df_processed['Plot_Time'])\n",
    "df_processed = df_processed.set_index('Plot_Time')\n",
    "\n",
    "# --- *** ADDED: Calculate Water Vapor Concentration (ppm) *** ---\n",
    "# Constants\n",
    "ATM_TO_KPA = 101.325 # Conversion factor from atm to kPa\n",
    "\n",
    "# Ensure required columns exist\n",
    "required_cols_for_ppm = ['SHT_TempC', 'SHT_Hum_%', 'BMP_PressAtm']\n",
    "if all(col in df_processed.columns for col in required_cols_for_ppm):\n",
    "    print(\"Calculating Water Vapor Concentration (H2O ppm)...\")\n",
    "    # Extract data (use .values for slight speedup with numpy, though pandas ops are fine too)\n",
    "    T_C = df_processed['SHT_TempC']\n",
    "    RH_percent = df_processed['SHT_Hum_%']\n",
    "    P_atm = df_processed['BMP_PressAtm']\n",
    "\n",
    "    # 1. Calculate Saturation Vapor Pressure (es) in kPa using Magnus-Tetens approximation\n",
    "    # es(T) = 0.6108 * exp((17.27 * T) / (T + 237.3))  (T in Celsius, es in kPa)\n",
    "    es_kPa = 0.6108 * np.exp((17.27 * T_C) / (T_C + 237.3))\n",
    "\n",
    "    # 2. Calculate Actual Vapor Pressure (e) in kPa\n",
    "    # e = (RH / 100) * es\n",
    "    e_kPa = (RH_percent / 100.0) * es_kPa\n",
    "\n",
    "    # 3. Convert Total Atmospheric Pressure (P) to kPa\n",
    "    P_total_kPa = P_atm * ATM_TO_KPA\n",
    "\n",
    "    # 4. Calculate Water Vapor Concentration in ppm (volume/volume)\n",
    "    # ppm_v = (e / P_total) * 1,000,000\n",
    "    # Ensure P_total_kPa is not zero to avoid division errors (though unlikely for atm pressure)\n",
    "    # Replace potential division by zero or NaN pressure with NaN result\n",
    "    with np.errstate(divide='ignore', invalid='ignore'): # Suppress warnings for division by zero/NaN\n",
    "         df_processed['H2O_ppm'] = (e_kPa / P_total_kPa) * 1e6\n",
    "         df_processed.loc[P_total_kPa <= 0, 'H2O_ppm'] = np.nan # Handle zero/negative pressure explicitly\n",
    "\n",
    "    print(\"H2O ppm calculation complete.\")\n",
    "    h2o_ppm_calculated = True\n",
    "else:\n",
    "    print(\"Warning: Cannot calculate H2O ppm. Missing one or more required columns: \", required_cols_for_ppm)\n",
    "    h2o_ppm_calculated = False\n",
    "    # Add a dummy column with NaNs if needed later, although outlier/aggregation will handle absence\n",
    "    if 'H2O_ppm' not in df_processed.columns:\n",
    "         df_processed['H2O_ppm'] = np.nan\n",
    "# --- *** END ADDED SECTION *** ---\n",
    "\n",
    "\n",
    "# Columns to process (updated to include H2O_ppm if calculated)\n",
    "# We still keep SHT_Hum_% here if needed for other potential processing,\n",
    "# but we won't plot it in the time series below. Add H2O_ppm.\n",
    "# --- *** MODIFIED: Added BMP_TempC and CPU_TempC for aggregation *** ---\n",
    "sensor_columns = ['SHT_TempC', 'SHT_Hum_%', 'BMP_PressAtm', 'CO2_ppm', 'BMP_TempC', 'CPU_TempC']\n",
    "# --- *** END MODIFIED SECTION *** ---\n",
    "if h2o_ppm_calculated and 'H2O_ppm' not in sensor_columns:\n",
    "     sensor_columns.append('H2O_ppm') # Add the new column for processing\n",
    "\n",
    "# 1. Outlier Removal (using IQR method per column)\n",
    "print(\"Removing outliers...\")\n",
    "for col in sensor_columns:\n",
    "    if col in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[col]): # Check if column exists and is numeric\n",
    "        Q1 = df_processed[col].quantile(0.25)\n",
    "        Q3 = df_processed[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Replace outliers with NaN. NaNs will be ignored by mean/std in resampling.\n",
    "        original_count = df_processed[col].count() # Count non-NaN before masking\n",
    "        df_processed[col] = df_processed[col].mask(\n",
    "            (df_processed[col] < lower_bound) | (df_processed[col] > upper_bound)\n",
    "        )\n",
    "        nan_count = original_count - df_processed[col].count() # Count NaNs introduced\n",
    "        # The original outlier calculation was slightly off, comparing df_plotting to df_processed bounds\n",
    "        # This version calculates outliers identified within df_processed itself\n",
    "        print(f\"Identified and marked {nan_count} outliers as NaN in '{col}'.\")\n",
    "    elif col not in df_processed.columns:\n",
    "         print(f\"Skipping outlier removal for '{col}': Column not found.\")\n",
    "    else:\n",
    "         print(f\"Skipping outlier removal for '{col}': Column is not numeric.\")\n",
    "\n",
    "\n",
    "# 2. Averaging for 1 minute and calculating standard deviation\n",
    "print(\"Aggregating data to 1-minute intervals (mean and std)...\")\n",
    "# The '1T' or '1min' string means 1-minute frequency for resampling.\n",
    "# .agg will compute both mean and std. NaN values are skipped by default.\n",
    "# Ensure we only try to aggregate columns that actually exist in df_processed\n",
    "columns_to_aggregate = [col for col in sensor_columns if col in df_processed.columns]\n",
    "if columns_to_aggregate:\n",
    "    df_aggregated = df_processed[columns_to_aggregate].resample('2min').agg(['mean', 'std'])\n",
    "else:\n",
    "    print(\"Error: No columns available for aggregation.\")\n",
    "    # Handle error appropriately, maybe exit or create an empty df_aggregated\n",
    "    df_aggregated = pd.DataFrame() # Create empty df to avoid downstream errors\n",
    "\n",
    "# df_aggregated will now have multi-level columns, e.g., ('SHT_TempC', 'mean') and ('SHT_TempC', 'std')\n",
    "\n",
    "# --- 1) Time Series Subplots (Modified) ---\n",
    "print(\"Generating Plot 1: Time Series Data (2-min average with std bands)...\")\n",
    "fig1, axes1 = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "fig1.suptitle('Sensor Time Series Data (2-min Average ±1 Std Dev)', fontsize=16, y=0.99)\n",
    "\n",
    "# --- *** MODIFIED: Plot Configuration *** ---\n",
    "# Replace SHT_Hum_% plot config with H2O_ppm config\n",
    "# Note: BMP_TempC and CPU_TempC are added directly to the first plot, not via this config list\n",
    "plot_configs = [\n",
    "    {'col': 'SHT_TempC', 'label': 'SHT Temp', 'color': 'crimson', 'ylabel': 'Temperature (°C)', 'title': 'Ambient & Sensor Temperatures'}, # MODIFIED Title\n",
    "    {'col': 'H2O_ppm', 'label': 'Water Vapor', 'color': 'deepskyblue', 'ylabel': 'H2O (ppm)', 'title': 'Water Vapor Concentration'}, # MODIFIED ROW\n",
    "    {'col': 'BMP_PressAtm', 'label': 'BMP Pressure', 'color': 'green', 'ylabel': 'Pressure (atm)', 'title': 'Atmospheric Pressure (BMP585)'},\n",
    "    {'col': 'CO2_ppm', 'label': 'CO2', 'color': 'purple', 'ylabel': 'CO2 (ppm)', 'title': 'CO2 Concentration (SCD30)'}\n",
    "]\n",
    "# --- *** END MODIFIED SECTION *** ---\n",
    "\n",
    "\n",
    "for i, config in enumerate(plot_configs):\n",
    "    ax = axes1[i]\n",
    "    col_name = config['col']\n",
    "\n",
    "    # Check if the specific column (mean and std) exists in the aggregated data\n",
    "    mean_col = (col_name, 'mean')\n",
    "    std_col = (col_name, 'std')\n",
    "\n",
    "    if mean_col not in df_aggregated.columns or std_col not in df_aggregated.columns:\n",
    "        # Provide more specific feedback if the column wasn't calculated or aggregated\n",
    "        if col_name == 'H2O_ppm' and not h2o_ppm_calculated:\n",
    "             print(f\"Warning: Cannot plot {col_name}. Data was not calculated (missing inputs).\")\n",
    "             ax.set_title(f\"{config['title']} (Input Data Missing)\")\n",
    "        elif col_name not in columns_to_aggregate:\n",
    "             print(f\"Warning: Cannot plot {col_name}. Column was not found in processed data before aggregation.\")\n",
    "             ax.set_title(f\"{config['title']} (Column Not Found)\")\n",
    "        else:\n",
    "             print(f\"Warning: Mean or std for {col_name} not found in aggregated data. Skipping plot.\")\n",
    "             ax.set_title(f\"{config['title']} (Aggregation Error?)\")\n",
    "\n",
    "        ax.set_ylabel(config['ylabel'])\n",
    "        if i == len(plot_configs) - 1: ax.set_xlabel('Time')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6) # Add grid for readability\n",
    "        continue\n",
    "\n",
    "    # Extract mean and std series\n",
    "    mean_series = df_aggregated[mean_col].dropna() # Drop rows where mean is NaN\n",
    "    std_series = df_aggregated[std_col].reindex(mean_series.index).fillna(0) # Align and fill NaN std with 0\n",
    "\n",
    "    if mean_series.empty:\n",
    "        print(f\"Warning: No data to plot for {col_name} after aggregation and NaN removal.\")\n",
    "        ax.set_title(f\"{config['title']} (No data after aggregation)\")\n",
    "        ax.set_ylabel(config['ylabel'])\n",
    "        if i == len(plot_configs) - 1: ax.set_xlabel('Time')\n",
    "        ax.grid(True, linestyle='--', alpha=0.6)\n",
    "        continue\n",
    "\n",
    "    # Plot the mean for the primary variable of this subplot\n",
    "    ax.plot(mean_series.index, mean_series, label=config['label'], color=config['color'], linewidth=1.5)\n",
    "\n",
    "    # Plot the standard deviation bands ONLY for the primary variable\n",
    "    ax.fill_between(mean_series.index,\n",
    "                    mean_series - std_series,\n",
    "                    mean_series + std_series,\n",
    "                    color=config['color'],\n",
    "                    alpha=0.2, # Semi-transparent\n",
    "                    label='±1 Std Dev' if i == 0 else \"_nolegend_\") # Label std dev band only once for SHT_TempC\n",
    "\n",
    "    # --- *** ADDED: Plot BMP_TempC and CPU_TempC on the first subplot (i=0) *** ---\n",
    "    if i == 0:\n",
    "        # Plot BMP_TempC mean if available\n",
    "        bmp_temp_mean_col = ('BMP_TempC', 'mean')\n",
    "        if bmp_temp_mean_col in df_aggregated.columns:\n",
    "            bmp_temp_mean_series = df_aggregated[bmp_temp_mean_col].dropna()\n",
    "            if not bmp_temp_mean_series.empty:\n",
    "                ax.plot(bmp_temp_mean_series.index, bmp_temp_mean_series, label='BMP Temp', color='orange', linewidth=1.0, linestyle='--')\n",
    "            else:\n",
    "                print(f\"Warning: No data to plot for BMP_TempC after aggregation and NaN removal.\")\n",
    "        else:\n",
    "            print(f\"Warning: BMP_TempC mean not found in aggregated data. Skipping its plot line.\")\n",
    "\n",
    "        # Plot CPU_TempC mean if available\n",
    "        cpu_temp_mean_col = ('CPU_TempC', 'mean')\n",
    "        if cpu_temp_mean_col in df_aggregated.columns:\n",
    "            cpu_temp_mean_series = df_aggregated[cpu_temp_mean_col].dropna()\n",
    "            if not cpu_temp_mean_series.empty:\n",
    "                 ax.plot(cpu_temp_mean_series.index, cpu_temp_mean_series, label='CPU Temp', color='gray', linewidth=1.0, linestyle=':')\n",
    "            else:\n",
    "                print(f\"Warning: No data to plot for CPU_TempC after aggregation and NaN removal.\")\n",
    "        else:\n",
    "             print(f\"Warning: CPU_TempC mean not found in aggregated data. Skipping its plot line.\")\n",
    "    # --- *** END ADDED SECTION *** ---\n",
    "\n",
    "    ax.set_ylabel(config['ylabel'])\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_title(config['title'])\n",
    "    ax.grid(True, linestyle='--', alpha=0.6) # Add grid for readability\n",
    "\n",
    "# Set common x-label only for the last plot\n",
    "axes1[-1].set_xlabel('Time')\n",
    "\n",
    "# Apply date formatting to the x-axis\n",
    "if not df_aggregated.empty and pd.api.types.is_datetime64_any_dtype(df_aggregated.index):\n",
    "    fig1.autofmt_xdate()\n",
    "    # Use AutoDateLocator and ConciseDateFormatter for better automatic formatting\n",
    "    locator = mdates.AutoDateLocator(minticks=5, maxticks=10)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    axes1[-1].xaxis.set_major_locator(locator)\n",
    "    axes1[-1].xaxis.set_major_formatter(formatter)\n",
    "    # axes1[-1].xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M')) # Alternative explicit format\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97]) # Adjust rect to prevent title overlap\n",
    "\n",
    "# Ensure output folder exists before saving\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "plot1_path = os.path.join(output_folder, \"plot1_time_series_aggregated.png\")\n",
    "plt.savefig(plot1_path)\n",
    "print(f\"Saved: {plot1_path}\")\n",
    "plt.show()\n",
    "# plt.close(fig1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c436c-8cb1-4a25-a754-32cce7789ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c08cf14-eaeb-40b0-848c-0a3fc610bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae425556-577d-42ee-a516-3c019c1a9608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) 3D Coordinates Plot ---\n",
    "print(\"Generating Plot 3: 3D GPS Coordinates...\")\n",
    "gps_fixed_data = df_plotting[df_plotting['GPS_Fix'] == 1].copy()\n",
    "\n",
    "coord_cols = ['GPS_Lon', 'GPS_Lat', 'GPS_Alt_m']\n",
    "for col in coord_cols:\n",
    "    gps_fixed_data[col] = pd.to_numeric(gps_fixed_data[col], errors='coerce')\n",
    "\n",
    "gps_fixed_data.dropna(subset=coord_cols, inplace=True)\n",
    "\n",
    "gps_fixed_data = gps_fixed_data[~((gps_fixed_data['GPS_Lon'] == 0) & \\\n",
    "                                  (gps_fixed_data['GPS_Lat'] == 0) & \\\n",
    "                                  (gps_fixed_data['GPS_Alt_m'] == 0))]\n",
    "\n",
    "if not gps_fixed_data.empty and len(gps_fixed_data) > 1:\n",
    "    fig3 = plt.figure(figsize=(10, 8))\n",
    "    ax3d = fig3.add_subplot(111, projection='3d')\n",
    "    \n",
    "    ax3d.plot(gps_fixed_data['GPS_Lon'], \n",
    "              gps_fixed_data['GPS_Lat'], \n",
    "              gps_fixed_data['GPS_Alt_m'], \n",
    "              label='GPS Track', marker='o', markersize=2, linestyle='-')\n",
    "    \n",
    "    ax3d.scatter(gps_fixed_data['GPS_Lon'].iloc[0], gps_fixed_data['GPS_Lat'].iloc[0], gps_fixed_data['GPS_Alt_m'].iloc[0],\n",
    "                 color='green', s=50, label='Start', depthshade=True)\n",
    "    ax3d.scatter(gps_fixed_data['GPS_Lon'].iloc[-1], gps_fixed_data['GPS_Lat'].iloc[-1], gps_fixed_data['GPS_Alt_m'].iloc[-1],\n",
    "                 color='red', s=50, label='End', depthshade=True)\n",
    "\n",
    "    ax3d.set_xlabel('Longitude (°)')\n",
    "    ax3d.set_ylabel('Latitude (°)')\n",
    "    ax3d.set_zlabel('Altitude (m)')\n",
    "    ax3d.set_title('3D GPS Track (where Fix=1)', fontsize=16)\n",
    "    ax3d.legend()\n",
    "    \n",
    "    x_range = gps_fixed_data['GPS_Lon'].max() - gps_fixed_data['GPS_Lon'].min()\n",
    "    y_range = gps_fixed_data['GPS_Lat'].max() - gps_fixed_data['GPS_Lat'].min()\n",
    "    z_range = gps_fixed_data['GPS_Alt_m'].max() - gps_fixed_data['GPS_Alt_m'].min()\n",
    "    max_range = max(x_range, y_range, z_range if z_range > 0 else (x_range+y_range)/2 if (x_range+y_range)>0 else 1 ) \n",
    "    if max_range == 0: max_range = 1 \n",
    "\n",
    "    mid_x = (gps_fixed_data['GPS_Lon'].max() + gps_fixed_data['GPS_Lon'].min()) * 0.5\n",
    "    mid_y = (gps_fixed_data['GPS_Lat'].max() + gps_fixed_data['GPS_Lat'].min()) * 0.5\n",
    "    mid_z = (gps_fixed_data['GPS_Alt_m'].max() + gps_fixed_data['GPS_Alt_m'].min()) * 0.5\n",
    "    \n",
    "    if x_range > 0 : ax3d.set_xlim(mid_x - max_range * 0.5, mid_x + max_range * 0.5)\n",
    "    if y_range > 0 : ax3d.set_ylim(mid_y - max_range * 0.5, mid_y + max_range * 0.5)\n",
    "    if z_range > 0 : ax3d.set_zlim(mid_z - max_range * 0.5, mid_z + max_range * 0.5)\n",
    "    elif 'GPS_Alt_m' in gps_fixed_data and not gps_fixed_data['GPS_Alt_m'].empty:\n",
    "        alt_min = gps_fixed_data['GPS_Alt_m'].min()\n",
    "        alt_max = gps_fixed_data['GPS_Alt_m'].max()\n",
    "        if alt_min == alt_max:\n",
    "             ax3d.set_zlim(alt_min - 1, alt_max + 1) # Provide a small range if all altitudes are same\n",
    "        else: # Should be covered by z_range > 0 but as a fallback\n",
    "             ax3d.set_zlim(alt_min, alt_max)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plot3_path = os.path.join(output_folder, \"plot3_3d_coordinates.png\")\n",
    "    plt.savefig(plot3_path)\n",
    "    print(f\"Saved: {plot3_path}\")\n",
    "    plt.show() # MODIFIED: Show plot\n",
    "    # plt.close(fig3) # MODIFIED: Commented out\n",
    "elif gps_fixed_data.empty:\n",
    "    print(\"No data with GPS_Fix=1 and valid coordinates found. Skipping 3D coordinate plot.\")\n",
    "else: # Only one point with GPS fix\n",
    "    print(\"Only one data point with GPS_Fix=1 and valid coordinates found. Skipping 3D line plot, plotting single point.\")\n",
    "    fig3 = plt.figure(figsize=(10, 8))\n",
    "    ax3d = fig3.add_subplot(111, projection='3d')\n",
    "    ax3d.scatter(gps_fixed_data['GPS_Lon'].iloc[0], gps_fixed_data['GPS_Lat'].iloc[0], gps_fixed_data['GPS_Alt_m'].iloc[0],\n",
    "                 color='blue', s=50, label='Single GPS Point', depthshade=True)\n",
    "    ax3d.set_xlabel('Longitude (°)')\n",
    "    ax3d.set_ylabel('Latitude (°)')\n",
    "    ax3d.set_zlabel('Altitude (m)')\n",
    "    ax3d.set_title('3D GPS Point (where Fix=1)', fontsize=16)\n",
    "    ax3d.legend()\n",
    "    plt.tight_layout()\n",
    "    plot3_path = os.path.join(output_folder, \"plot3_3d_coordinates.png\")\n",
    "    plt.savefig(plot3_path)\n",
    "    print(f\"Saved: {plot3_path}\")\n",
    "    plt.show() # MODIFIED: Show plot\n",
    "    # plt.close(fig3) # MODIFIED: Commented out\n",
    "\n",
    "print(\"\\nFinished generating and saving plots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a440109d-8fcb-4c23-ae46-c50ad615e965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
